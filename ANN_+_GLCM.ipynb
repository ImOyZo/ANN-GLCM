{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RSuRSFBWu3k",
        "outputId": "ae7f10a8-fe33-49cc-b2ff-7a3d0a34f755"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-image\n",
        "\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "from skimage.util import img_as_ubyte\n",
        "from skimage.color import rgb2gray  # New import\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def calculate_glcm(image, dx=1, dy=0):\n",
        "    \"\"\"Calculate GLCM for an image (handles both grayscale and RGB)\"\"\"\n",
        "    image_array = np.array(image)\n",
        "    if len(image_array.shape) == 3:\n",
        "        image_array = rgb2gray(image_array)\n",
        "    channel = img_as_ubyte(image_array)\n",
        "    return graycomatrix(channel, distances=[dx], angles=[dy], symmetric=True, normed=True)\n",
        "\n",
        "def extract_glcm_features(glcm):\n",
        "    \"\"\"Extract features from GLCM\"\"\"\n",
        "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
        "    dissimilarity = graycoprops(glcm, 'dissimilarity')[0, 0]\n",
        "    homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n",
        "    energy = graycoprops(glcm, 'energy')[0, 0]\n",
        "    correlation = graycoprops(glcm, 'correlation')[0, 0]\n",
        "    return [contrast, dissimilarity, homogeneity, energy, correlation]\n",
        "\n",
        "def extract_rgb_features(image):\n",
        "    \"\"\"Extract basic RGB statistics\"\"\"\n",
        "    arr = np.array(image)\n",
        "    features = []\n",
        "    for i in range(3):  # R, G, B channels\n",
        "        channel = arr[:,:,i]\n",
        "        features.extend([np.mean(channel), np.std(channel), np.median(channel)])\n",
        "    return features\n",
        "\n",
        "def extract_glcm_rgb_features(image):\n",
        "    \"\"\"Combine GLCM and RGB features\"\"\"\n",
        "    glcm = calculate_glcm(image)\n",
        "    glcm_feats = extract_glcm_features(glcm)\n",
        "    rgb_feats = extract_rgb_features(image)\n",
        "    return glcm_feats + rgb_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-yEqyS_9eFA",
        "outputId": "9e2b077a-4408-4b85-8795-ae7f30f03cb8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class_paths = {\n",
        "    0: (\"tiger\", \"/content/drive/MyDrive/Dataset/tiger\"),\n",
        "    1: (\"lion\", \"/content/drive/MyDrive/Dataset/lion\"),\n",
        "    2: (\"cheetah\", \"/content/drive/MyDrive/Dataset/cheetah\"),\n",
        "    3: (\"leopard\", \"/content/drive/MyDrive/Dataset/leopard\")\n",
        "}\n",
        "\n",
        "\n",
        "class_names = [v[0] for v in sorted(class_paths.values())]\n",
        "dataset_paths = [v[1] for v in sorted(class_paths.values())]\n",
        "\n",
        "\n",
        "def load_dataset(paths, img_size=(256, 256)):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    for label, folder_path in enumerate(paths):\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
        "                try:\n",
        "                    image_path = os.path.join(folder_path, filename)\n",
        "                    image = Image.open(image_path).convert(\"RGB\").resize(img_size)\n",
        "                    features = extract_glcm_rgb_features(image)\n",
        "                    all_features.append(features)\n",
        "                    all_labels.append(label)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {filename}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "    return np.array(all_features), np.array(all_labels)\n",
        "\n",
        "X, y = load_dataset(dataset_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysPizQz9srTo",
        "outputId": "10934f10-8bdc-4a7e-9d22-92991ab0d9c4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)  # Changed from 'sparse' to 'sparse_output'\n",
        "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_onehot, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M7BDOCXTb0Ne"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        prev_size = input_size\n",
        "        for size in hidden_sizes:\n",
        "            self.weights.append(np.random.randn(prev_size, size) * 0.1)\n",
        "            self.biases.append(np.zeros(size))\n",
        "            prev_size = size\n",
        "\n",
        "        # Output layer\n",
        "        self.weights.append(np.random.randn(prev_size, output_size) * 0.1)\n",
        "        self.biases.append(np.zeros(output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.activations = [x]\n",
        "        self.z_values = []\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            activation = self.softmax(z) if i == len(self.weights)-1 else self.sigmoid(z)\n",
        "            self.activations.append(activation)\n",
        "\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def backward(self, x, y, learning_rate):\n",
        "        output = self.forward(x)\n",
        "        error = output - y\n",
        "        deltas = [error]\n",
        "\n",
        "        # Backpropagate\n",
        "        for i in range(len(self.weights)-1, 0, -1):\n",
        "            delta = np.dot(deltas[-1], self.weights[i].T) * self.sigmoid_derivative(self.activations[i])\n",
        "            deltas.append(delta)\n",
        "\n",
        "        deltas.reverse()\n",
        "\n",
        "        # Update weights and biases\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= learning_rate * np.dot(self.activations[i].T, deltas[i])\n",
        "            self.biases[i] -= learning_rate * np.sum(deltas[i], axis=0)\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate, validation_data=None):\n",
        "        for epoch in range(epochs):\n",
        "            self.backward(X, y, learning_rate)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                output = self.forward(X)\n",
        "                loss = -np.mean(y * np.log(output + 1e-9))\n",
        "\n",
        "                if validation_data:\n",
        "                    X_val, y_val = validation_data\n",
        "                    val_pred = self.predict_class(X_val)\n",
        "                    val_true = np.argmax(y_val, axis=1)\n",
        "                    val_acc = np.mean(val_pred == val_true)\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}, Val Acc: {val_acc:.2%}\")\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict_class(self, X):\n",
        "        proba = self.forward(X)\n",
        "        return np.argmax(proba, axis=1)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        model_data = {\n",
        "            'weights': [w.tolist() for w in self.weights],\n",
        "            'biases': [b.tolist() for b in self.biases],\n",
        "            'input_size': self.weights[0].shape[0],\n",
        "            'hidden_sizes': [w.shape[1] for w in self.weights[:-1]],\n",
        "            'output_size': self.weights[-1].shape[1]\n",
        "        }\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(model_data, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_model(filename):\n",
        "        with open(filename, 'r') as f:\n",
        "            model_data = json.load(f)\n",
        "\n",
        "        model = NeuralNetwork(\n",
        "            model_data['input_size'],\n",
        "            model_data['hidden_sizes'],\n",
        "            model_data['output_size']\n",
        "        )\n",
        "\n",
        "        model.weights = [np.array(w) for w in model_data['weights']]\n",
        "        model.biases = [np.array(b) for b in model_data['biases']]\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D4rM6EREy6Yh"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from IPython.display import display\n",
        "\n",
        "def extract_features_from_image(image_path, img_size=(256, 256)):\n",
        "    \"\"\"Extract features from a single image\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\").resize(img_size)\n",
        "    return extract_glcm_rgb_features(image)\n",
        "\n",
        "def predict_image(image_path, model, feature_mean, feature_std, class_names):\n",
        "    \"\"\"Make prediction on a single image\"\"\"\n",
        "    features = extract_features_from_image(image_path)\n",
        "    features_normalized = (features - feature_mean) / (feature_std + 1e-6)\n",
        "    proba = model.predict_proba(features_normalized.reshape(1, -1))[0]\n",
        "    pred_class = np.argmax(proba)\n",
        "    return pred_class, proba, class_names[pred_class]\n",
        "\n",
        "def interactive_prediction(model, feature_mean, feature_std, class_names):\n",
        "    \"\"\"Upload and predict images interactively\"\"\"\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        print(f\"\\nPredicting: {filename}\")\n",
        "        img = Image.open(filename)\n",
        "        display(img.resize((200, 200)))\n",
        "\n",
        "        pred_class, proba, class_name = predict_image(\n",
        "            filename, model, feature_mean, feature_std, class_names)\n",
        "\n",
        "        print(f\"Predicted class: {class_name}\")\n",
        "        print(\"Class probabilities:\")\n",
        "        for i, name in enumerate(class_names):\n",
        "            print(f\"  {name}: {proba[i]:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92nlc9iC2deE",
        "outputId": "6cd22798-520a-47e9-c74f-afb4ef5d50c2"
      },
      "outputs": [],
      "source": [
        "# Initialize and train network\n",
        "input_size = X_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "ann = NeuralNetwork(input_size, [64, 32], output_size)\n",
        "\n",
        "ann.train(\n",
        "    X_train, y_train,\n",
        "    epochs=30000,\n",
        "    learning_rate=0.0001,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "\n",
        "# Save model and scaler\n",
        "ann.save_model('ann_glcm_model.json')\n",
        "np.savez('scaler_params.npz', mean=scaler.mean_, scale=scaler.scale_)\n",
        "\n",
        "# Evaluate\n",
        "test_preds = ann.predict_class(X_test)\n",
        "test_true = np.argmax(y_test, axis=1)\n",
        "accuracy = np.mean(test_preds == test_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "PFeJFsGVXIRV",
        "outputId": "55ce9814-23c6-4cff-fef8-add50596ca6c"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    from sklearn.metrics import classification_report\n",
        "    print(\"\\nTest Accuracy: {:.2%}\".format(accuracy))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(test_true, test_preds, target_names=class_names))\n",
        "\n",
        "    print(\"\\nStarting interactive prediction...\")\n",
        "    from google.colab import files\n",
        "    interactive_prediction(ann, scaler.mean_, scaler.scale_, class_names)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
